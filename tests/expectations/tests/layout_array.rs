/* automatically generated by rust-bindgen */


#![allow(non_snake_case)]


pub const RTE_CACHE_LINE_SIZE: ::std::os::raw::c_uint = 64;
pub const RTE_MEMPOOL_OPS_NAMESIZE: ::std::os::raw::c_uint = 32;
pub const RTE_MEMPOOL_MAX_OPS_IDX: ::std::os::raw::c_uint = 16;
pub const RTE_HEAP_NUM_FREELISTS: ::std::os::raw::c_uint = 13;
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct rte_mempool([u8; 0]);
/**
 * Prototype for implementation specific data provisioning function.
 *
 * The function should provide the implementation specific memory for
 * for use by the other mempool ops functions in a given mempool ops struct.
 * E.g. the default ops provides an instance of the rte_ring for this purpose.
 * it will most likely point to a different type of data structure, and
 * will be transparent to the application programmer.
 * This function should set mp->pool_data.
 */
pub type rte_mempool_alloc_t =
    ::std::option::Option<unsafe extern "C" fn(mp: *mut rte_mempool)
                              -> ::std::os::raw::c_int>;
/**
 * Free the opaque private data pointed to by mp->pool_data pointer.
 */
pub type rte_mempool_free_t =
    ::std::option::Option<unsafe extern "C" fn(mp: *mut rte_mempool)>;
/**
 * Enqueue an object into the external pool.
 */
pub type rte_mempool_enqueue_t =
    ::std::option::Option<unsafe extern "C" fn(mp: *mut rte_mempool,
                                               obj_table:
                                                   *const *const ::std::os::raw::c_void,
                                               n: ::std::os::raw::c_uint)
                              -> ::std::os::raw::c_int>;
/**
 * Dequeue an object from the external pool.
 */
pub type rte_mempool_dequeue_t =
    ::std::option::Option<unsafe extern "C" fn(mp: *mut rte_mempool,
                                               obj_table:
                                                   *mut *mut ::std::os::raw::c_void,
                                               n: ::std::os::raw::c_uint)
                              -> ::std::os::raw::c_int>;
/**
 * Return the number of available objects in the external pool.
 */
pub type rte_mempool_get_count =
    ::std::option::Option<unsafe extern "C" fn(mp: *const rte_mempool)
                              -> ::std::os::raw::c_uint>;
/** Structure defining mempool operations structure */
#[repr(C)]
#[derive(Debug, Copy)]
pub struct rte_mempool_ops {
    /**< Name of mempool ops struct. */
    pub name: [::std::os::raw::c_char; 32usize],
    /**< Allocate private data. */
    pub alloc: rte_mempool_alloc_t,
    /**< Free the external pool. */
    pub free: rte_mempool_free_t,
    /**< Enqueue an object. */
    pub enqueue: rte_mempool_enqueue_t,
    /**< Dequeue an object. */
    pub dequeue: rte_mempool_dequeue_t,
    /**< Get qty of available objs. */
    pub get_count: rte_mempool_get_count,
    pub __bindgen_padding_0: [u64; 7usize],
}
#[test]
fn bindgen_test_layout_rte_mempool_ops() {
    assert_eq!(::std::mem::size_of::<rte_mempool_ops>() , 128usize);
    assert_eq! (unsafe {
                & ( * ( 0 as * const rte_mempool_ops ) ) . name as * const _
                as usize } , 0usize);
    assert_eq! (unsafe {
                & ( * ( 0 as * const rte_mempool_ops ) ) . alloc as * const _
                as usize } , 32usize);
    assert_eq! (unsafe {
                & ( * ( 0 as * const rte_mempool_ops ) ) . free as * const _
                as usize } , 40usize);
    assert_eq! (unsafe {
                & ( * ( 0 as * const rte_mempool_ops ) ) . enqueue as * const
                _ as usize } , 48usize);
    assert_eq! (unsafe {
                & ( * ( 0 as * const rte_mempool_ops ) ) . dequeue as * const
                _ as usize } , 56usize);
    assert_eq! (unsafe {
                & ( * ( 0 as * const rte_mempool_ops ) ) . get_count as *
                const _ as usize } , 64usize);
}
impl Clone for rte_mempool_ops {
    fn clone(&self) -> Self { *self }
}
/**
 * The rte_spinlock_t type.
 */
#[repr(C)]
#[derive(Debug, Copy)]
pub struct _bindgen_ty_1 {
    /**< lock status 0 = unlocked, 1 = locked */
    pub locked: ::std::os::raw::c_int,
}
#[test]
fn bindgen_test_layout__bindgen_ty_1() {
    assert_eq!(::std::mem::size_of::<_bindgen_ty_1>() , 4usize);
    assert_eq! (::std::mem::align_of::<_bindgen_ty_1>() , 4usize);
    assert_eq! (unsafe {
                & ( * ( 0 as * const _bindgen_ty_1 ) ) . locked as * const _
                as usize } , 0usize);
}
impl Clone for _bindgen_ty_1 {
    fn clone(&self) -> Self { *self }
}
pub type rte_spinlock_t = _bindgen_ty_1;
/**
 * Structure storing the table of registered ops structs, each of which contain
 * the function pointers for the mempool ops functions.
 * Each process has its own storage for this ops struct array so that
 * the mempools can be shared across primary and secondary processes.
 * The indices used to access the array are valid across processes, whereas
 * any function pointers stored directly in the mempool struct would not be.
 * This results in us simply having "ops_index" in the mempool struct.
 */
#[repr(C)]
#[derive(Debug, Copy)]
pub struct rte_mempool_ops_table {
    /**< Spinlock for add/delete. */
    pub sl: rte_spinlock_t,
    /**< Number of used ops structs in the table. */
    pub num_ops: u32,
    pub __bindgen_padding_0: [u64; 7usize],
    /**
	 * Storage for all possible ops structs.
	 */
    pub ops: [rte_mempool_ops; 16usize],
}
#[test]
fn bindgen_test_layout_rte_mempool_ops_table() {
    assert_eq!(::std::mem::size_of::<rte_mempool_ops_table>() , 2112usize);
    assert_eq! (unsafe {
                & ( * ( 0 as * const rte_mempool_ops_table ) ) . sl as * const
                _ as usize } , 0usize);
    assert_eq! (unsafe {
                & ( * ( 0 as * const rte_mempool_ops_table ) ) . num_ops as *
                const _ as usize } , 4usize);
    assert_eq! (unsafe {
                & ( * ( 0 as * const rte_mempool_ops_table ) ) . ops as *
                const _ as usize } , 64usize);
}
impl Clone for rte_mempool_ops_table {
    fn clone(&self) -> Self { *self }
}
/**
 * Structure to hold malloc heap
 */
#[repr(C)]
#[derive(Debug, Copy)]
pub struct malloc_heap {
    pub lock: rte_spinlock_t,
    pub free_head: [malloc_heap__bindgen_ty_1; 13usize],
    pub alloc_count: ::std::os::raw::c_uint,
    pub total_size: usize,
}
#[repr(C)]
#[derive(Debug, Copy)]
pub struct malloc_heap__bindgen_ty_1 {
    pub lh_first: *mut malloc_heap__bindgen_ty_1_malloc_elem,
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct malloc_heap__bindgen_ty_1_malloc_elem([u8; 0]);
#[test]
fn bindgen_test_layout_malloc_heap__bindgen_ty_1() {
    assert_eq!(::std::mem::size_of::<malloc_heap__bindgen_ty_1>() , 8usize);
    assert_eq! (::std::mem::align_of::<malloc_heap__bindgen_ty_1>() , 8usize);
    assert_eq! (unsafe {
                & ( * ( 0 as * const malloc_heap__bindgen_ty_1 ) ) . lh_first
                as * const _ as usize } , 0usize);
}
impl Clone for malloc_heap__bindgen_ty_1 {
    fn clone(&self) -> Self { *self }
}
#[test]
fn bindgen_test_layout_malloc_heap() {
    assert_eq!(::std::mem::size_of::<malloc_heap>() , 128usize);
    assert_eq! (unsafe {
                & ( * ( 0 as * const malloc_heap ) ) . lock as * const _ as
                usize } , 0usize);
    assert_eq! (unsafe {
                & ( * ( 0 as * const malloc_heap ) ) . free_head as * const _
                as usize } , 8usize);
    assert_eq! (unsafe {
                & ( * ( 0 as * const malloc_heap ) ) . alloc_count as * const
                _ as usize } , 112usize);
    assert_eq! (unsafe {
                & ( * ( 0 as * const malloc_heap ) ) . total_size as * const _
                as usize } , 120usize);
}
impl Clone for malloc_heap {
    fn clone(&self) -> Self { *self }
}
